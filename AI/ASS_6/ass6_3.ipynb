{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ankushdeep Singh_102003174\n",
    "# ass6_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the Iris.csv file\n",
    "data = load_iris()\n",
    "x = data.data\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data.target\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x,y, random_state = 50, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST SIZE:  [0.07215328]\n",
      "Criterion=entropy , max_depth= 6 , min_samples_split= 5 , min_samples_leaf= 5 , max_leaf_nodes= 3\n",
      "Test data accuracy: 1.0\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "           1       1.00      1.00      1.00         3\n",
      "           2       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00        11\n",
      "   macro avg       1.00      1.00      1.00        11\n",
      "weighted avg       1.00      1.00      1.00        11\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 4 , min_samples_split= 8 , min_samples_leaf= 2 , max_leaf_nodes= 6\n",
      "Test data accuracy: 1.0\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "           1       1.00      1.00      1.00         3\n",
      "           2       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00        11\n",
      "   macro avg       1.00      1.00      1.00        11\n",
      "weighted avg       1.00      1.00      1.00        11\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 6 , min_samples_split= 8 , min_samples_leaf= 2 , max_leaf_nodes= 8\n",
      "Test data accuracy: 1.0\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "           1       1.00      1.00      1.00         3\n",
      "           2       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00        11\n",
      "   macro avg       1.00      1.00      1.00        11\n",
      "weighted avg       1.00      1.00      1.00        11\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 3 , min_samples_split= 10 , min_samples_leaf= 5 , max_leaf_nodes= 8\n",
      "Test data accuracy: 1.0\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "           1       1.00      1.00      1.00         3\n",
      "           2       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00        11\n",
      "   macro avg       1.00      1.00      1.00        11\n",
      "weighted avg       1.00      1.00      1.00        11\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 3 , min_samples_split= 4 , min_samples_leaf= 3 , max_leaf_nodes= 6\n",
      "Test data accuracy: 1.0\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "           1       1.00      1.00      1.00         3\n",
      "           2       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00        11\n",
      "   macro avg       1.00      1.00      1.00        11\n",
      "weighted avg       1.00      1.00      1.00        11\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 3 , min_samples_split= 10 , min_samples_leaf= 5 , max_leaf_nodes= 3\n",
      "Test data accuracy: 0.9090909090909091\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "           1       1.00      0.67      0.80         3\n",
      "           2       0.75      1.00      0.86         3\n",
      "\n",
      "    accuracy                           0.91        11\n",
      "   macro avg       0.92      0.89      0.89        11\n",
      "weighted avg       0.93      0.91      0.91        11\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 6 , min_samples_split= 8 , min_samples_leaf= 5 , max_leaf_nodes= 8\n",
      "Test data accuracy: 1.0\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "           1       1.00      1.00      1.00         3\n",
      "           2       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00        11\n",
      "   macro avg       1.00      1.00      1.00        11\n",
      "weighted avg       1.00      1.00      1.00        11\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 3 , min_samples_split= 8 , min_samples_leaf= 5 , max_leaf_nodes= 6\n",
      "Test data accuracy: 1.0\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "           1       1.00      1.00      1.00         3\n",
      "           2       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00        11\n",
      "   macro avg       1.00      1.00      1.00        11\n",
      "weighted avg       1.00      1.00      1.00        11\n",
      "\n",
      "\n",
      "TEST SIZE:  [0.22015485]\n",
      "Criterion=entropy , max_depth= 4 , min_samples_split= 8 , min_samples_leaf= 8 , max_leaf_nodes= 3\n",
      "Test data accuracy: 0.9705882352941176\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.97        34\n",
      "   macro avg       0.97      0.98      0.97        34\n",
      "weighted avg       0.97      0.97      0.97        34\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 3 , min_samples_split= 5 , min_samples_leaf= 2 , max_leaf_nodes= 3\n",
      "Test data accuracy: 0.9705882352941176\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.97        34\n",
      "   macro avg       0.97      0.98      0.97        34\n",
      "weighted avg       0.97      0.97      0.97        34\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 6 , min_samples_split= 8 , min_samples_leaf= 3 , max_leaf_nodes= 8\n",
      "Test data accuracy: 0.9705882352941176\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.97        34\n",
      "   macro avg       0.97      0.98      0.97        34\n",
      "weighted avg       0.97      0.97      0.97        34\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 3 , min_samples_split= 4 , min_samples_leaf= 8 , max_leaf_nodes= 8\n",
      "Test data accuracy: 0.9705882352941176\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.97        34\n",
      "   macro avg       0.97      0.98      0.97        34\n",
      "weighted avg       0.97      0.97      0.97        34\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 6 , min_samples_split= 8 , min_samples_leaf= 5 , max_leaf_nodes= 3\n",
      "Test data accuracy: 0.9705882352941176\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.97        34\n",
      "   macro avg       0.97      0.98      0.97        34\n",
      "weighted avg       0.97      0.97      0.97        34\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 4 , min_samples_split= 8 , min_samples_leaf= 3 , max_leaf_nodes= 6\n",
      "Test data accuracy: 0.9705882352941176\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.97        34\n",
      "   macro avg       0.97      0.98      0.97        34\n",
      "weighted avg       0.97      0.97      0.97        34\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 3 , min_samples_split= 4 , min_samples_leaf= 5 , max_leaf_nodes= 3\n",
      "Test data accuracy: 0.9705882352941176\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.97        34\n",
      "   macro avg       0.97      0.98      0.97        34\n",
      "weighted avg       0.97      0.97      0.97        34\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 6 , min_samples_split= 4 , min_samples_leaf= 5 , max_leaf_nodes= 4\n",
      "Test data accuracy: 0.9705882352941176\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.97        34\n",
      "   macro avg       0.97      0.98      0.97        34\n",
      "weighted avg       0.97      0.97      0.97        34\n",
      "\n",
      "\n",
      "TEST SIZE:  [0.35002052]\n",
      "Criterion=entropy , max_depth= 5 , min_samples_split= 10 , min_samples_leaf= 2 , max_leaf_nodes= 3\n",
      "Test data accuracy: 0.9705882352941176\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.97        34\n",
      "   macro avg       0.97      0.98      0.97        34\n",
      "weighted avg       0.97      0.97      0.97        34\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 5 , min_samples_split= 8 , min_samples_leaf= 5 , max_leaf_nodes= 6\n",
      "Test data accuracy: 0.9705882352941176\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.97        34\n",
      "   macro avg       0.97      0.98      0.97        34\n",
      "weighted avg       0.97      0.97      0.97        34\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 3 , min_samples_split= 10 , min_samples_leaf= 5 , max_leaf_nodes= 4\n",
      "Test data accuracy: 0.9705882352941176\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.97        34\n",
      "   macro avg       0.97      0.98      0.97        34\n",
      "weighted avg       0.97      0.97      0.97        34\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 5 , min_samples_split= 5 , min_samples_leaf= 8 , max_leaf_nodes= 3\n",
      "Test data accuracy: 0.9705882352941176\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.97        34\n",
      "   macro avg       0.97      0.98      0.97        34\n",
      "weighted avg       0.97      0.97      0.97        34\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 6 , min_samples_split= 8 , min_samples_leaf= 5 , max_leaf_nodes= 3\n",
      "Test data accuracy: 0.9705882352941176\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.97        34\n",
      "   macro avg       0.97      0.98      0.97        34\n",
      "weighted avg       0.97      0.97      0.97        34\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 3 , min_samples_split= 8 , min_samples_leaf= 8 , max_leaf_nodes= 3\n",
      "Test data accuracy: 0.9705882352941176\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.97        34\n",
      "   macro avg       0.97      0.98      0.97        34\n",
      "weighted avg       0.97      0.97      0.97        34\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 5 , min_samples_split= 4 , min_samples_leaf= 5 , max_leaf_nodes= 4\n",
      "Test data accuracy: 0.9705882352941176\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.97        34\n",
      "   macro avg       0.97      0.98      0.97        34\n",
      "weighted avg       0.97      0.97      0.97        34\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 6 , min_samples_split= 8 , min_samples_leaf= 8 , max_leaf_nodes= 3\n",
      "Test data accuracy: 0.9705882352941176\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.97        34\n",
      "   macro avg       0.97      0.98      0.97        34\n",
      "weighted avg       0.97      0.97      0.97        34\n",
      "\n",
      "\n",
      "TEST SIZE:  [0.02655837]\n",
      "Criterion=entropy , max_depth= 4 , min_samples_split= 4 , min_samples_leaf= 5 , max_leaf_nodes= 4\n",
      "Test data accuracy: 1.0\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         4\n",
      "   macro avg       1.00      1.00      1.00         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 6 , min_samples_split= 10 , min_samples_leaf= 2 , max_leaf_nodes= 4\n",
      "Test data accuracy: 1.0\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         4\n",
      "   macro avg       1.00      1.00      1.00         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 3 , min_samples_split= 10 , min_samples_leaf= 8 , max_leaf_nodes= 4\n",
      "Test data accuracy: 1.0\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         4\n",
      "   macro avg       1.00      1.00      1.00         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 3 , min_samples_split= 10 , min_samples_leaf= 3 , max_leaf_nodes= 8\n",
      "Test data accuracy: 1.0\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         4\n",
      "   macro avg       1.00      1.00      1.00         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 6 , min_samples_split= 8 , min_samples_leaf= 3 , max_leaf_nodes= 3\n",
      "Test data accuracy: 1.0\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         4\n",
      "   macro avg       1.00      1.00      1.00         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 4 , min_samples_split= 5 , min_samples_leaf= 8 , max_leaf_nodes= 6\n",
      "Test data accuracy: 1.0\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         4\n",
      "   macro avg       1.00      1.00      1.00         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 6 , min_samples_split= 8 , min_samples_leaf= 3 , max_leaf_nodes= 6\n",
      "Test data accuracy: 1.0\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         4\n",
      "   macro avg       1.00      1.00      1.00         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 3 , min_samples_split= 4 , min_samples_leaf= 2 , max_leaf_nodes= 3\n",
      "Test data accuracy: 1.0\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         4\n",
      "   macro avg       1.00      1.00      1.00         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n",
      "\n",
      "TEST SIZE:  [0.43799414]\n",
      "Criterion=entropy , max_depth= 4 , min_samples_split= 5 , min_samples_leaf= 3 , max_leaf_nodes= 6\n",
      "Test data accuracy: 1.0\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         4\n",
      "   macro avg       1.00      1.00      1.00         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 5 , min_samples_split= 10 , min_samples_leaf= 2 , max_leaf_nodes= 8\n",
      "Test data accuracy: 1.0\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         4\n",
      "   macro avg       1.00      1.00      1.00         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 3 , min_samples_split= 5 , min_samples_leaf= 5 , max_leaf_nodes= 6\n",
      "Test data accuracy: 1.0\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         4\n",
      "   macro avg       1.00      1.00      1.00         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 3 , min_samples_split= 5 , min_samples_leaf= 8 , max_leaf_nodes= 6\n",
      "Test data accuracy: 1.0\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         4\n",
      "   macro avg       1.00      1.00      1.00         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 5 , min_samples_split= 10 , min_samples_leaf= 8 , max_leaf_nodes= 6\n",
      "Test data accuracy: 1.0\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         4\n",
      "   macro avg       1.00      1.00      1.00         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 4 , min_samples_split= 8 , min_samples_leaf= 2 , max_leaf_nodes= 6\n",
      "Test data accuracy: 1.0\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         4\n",
      "   macro avg       1.00      1.00      1.00         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 6 , min_samples_split= 8 , min_samples_leaf= 2 , max_leaf_nodes= 8\n",
      "Test data accuracy: 1.0\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         4\n",
      "   macro avg       1.00      1.00      1.00         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n",
      "\n",
      "Criterion=entropy , max_depth= 5 , min_samples_split= 4 , min_samples_leaf= 2 , max_leaf_nodes= 3\n",
      "Test data accuracy: 1.0\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         4\n",
      "   macro avg       1.00      1.00      1.00         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    test_size=np.random.rand(1)\n",
    "    while test_size>0.5:\n",
    "        test_size=np.random.rand(1)\n",
    "        Test_size=(test_size[0])\n",
    "    print(\"TEST SIZE: \",test_size)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x,y, random_state = 50, test_size =Test_size)\n",
    "\n",
    "    max_depth=[3,4,5,6]\n",
    "    min_samples_spit=[8,5,10,4]\n",
    "    min_samples_leaf=[5,3,8,2]\n",
    "    max_leaf_nodes=[4,3,6,8]\n",
    "\n",
    "    for i in range(8):\n",
    "        d=random.randint(0,3)\n",
    "        s=random.randint(0,3)\n",
    "        l=random.randint(0,3)\n",
    "        n=random.randint(0,3)\n",
    "        clf = DecisionTreeClassifier(criterion='entropy',max_depth=max_depth[d],min_samples_split=min_samples_spit[s],min_samples_leaf=min_samples_leaf[l],max_leaf_nodes=max_leaf_nodes[n])\n",
    "        clf.fit(X_train,y_train)\n",
    "\n",
    "        # Predict Accuracy Score\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        print('Criterion=entropy',', max_depth=',max_depth[d],', min_samples_split=',min_samples_spit[s],', min_samples_leaf=',min_samples_leaf[l],', max_leaf_nodes=',max_leaf_nodes[n])\n",
    "        print(\"Test data accuracy:\",accuracy_score(y_test,y_pred))\n",
    "        print(\"Classification report: \",classification_report(y_test,y_pred))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
